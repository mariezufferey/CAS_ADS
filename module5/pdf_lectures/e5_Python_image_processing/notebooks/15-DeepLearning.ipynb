{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 15. Deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep learning methods are used more and more fequently for complex segmentation tasks. The basic idea of that approach is to let a system learn by itself what are the important features of the objects to segment by feeding it training examples.\n",
    "\n",
    "Of course you will not learn all the details about deep learning in this single notebook. The goal here is simply to give a very brief overview of the steps involved. In particular the goal is to show that if you are provided with a trained network *e.g.* by a collaborator, using it to segment your data is very straightforward.\n",
    "\n",
    "The example here uses Tensorflow and Keras. Tensorflow is Google's deep learning library that is widely used. Keras is a layer that sits on top of tools like Tensorflow and allows one to simplify the prototyping of a deep learning pipeline. It can also transparently be used with other \"backends\" like PyTorch, Facebook's deep learning library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.external.tifffile import TiffFile\n",
    "from skimage.measure import label, regionprops\n",
    "from skimage.segmentation import watershed\n",
    "\n",
    "#import your function\n",
    "import sys, os\n",
    "from course_functions import detect_nuclei\n",
    "\n",
    "if not os.path.isdir('MyData/DL'):\n",
    "    os.makedirs('MyData/DL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.1 Creating the training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple example, we are going to use the Zebra fish embryo *nuclei* that we have tried to segment before. Usually, one would create a training set by manually segmenting data or at least manually correcting them. Here we cheat and use our previous segmentation pipeline to create a learning dataset.\n",
    "\n",
    "First we have to decide how large our training images are going to be. This is set by the type of computing resource used and the memory size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "imsize = 64\n",
    "image_rows = 64\n",
    "image_cols = 64\n",
    "channels = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the image to process\n",
    "data = TiffFile('Data/30567/30567.tif')\n",
    "image = data.pages[0].asarray()\n",
    "per_image = np.floor(np.array(image.shape)/imsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create our training set, we are going to segment 5 images using our previous pipeline. Then we are going to cut the original image and its mask into 64x64 pieces. We exclude images which have no nuclei as they don't contain interesting information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/skimage/filters/rank/generic.py:102: UserWarning: Bitdepth of 14 may result in bad rank filter performance due to large number of bins.\n",
      "  \"performance due to large number of bins.\" % bitdepth)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fbdbc1573c8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_images = []\n",
    "all_masks = []\n",
    "for t in (3,13,23,33,43):\n",
    "    image = data.pages[t].asarray()\n",
    "    im_float = image.astype(np.float32)\n",
    "    #create your mask\n",
    "    nuclei = detect_nuclei(image)\n",
    "    nuclei = nuclei.astype(np.uint8)\n",
    "    \n",
    "    for i in range(int(per_image[0])):\n",
    "        for j in range(int(per_image[1])):\n",
    "            if np.sum(nuclei[i*imsize:(i+1)*imsize,j*imsize:(j+1)*imsize])>1:\n",
    "                all_images.append(im_float[i*imsize:(i+1)*imsize,j*imsize:(j+1)*imsize])\n",
    "                all_masks.append(nuclei[i*imsize:(i+1)*imsize,j*imsize:(j+1)*imsize])\n",
    "\n",
    "plt.imshow(nuclei, cmap = 'gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we could split our dataset into a training and testing set. We have enough other data so we use all examples for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283\n",
      "280\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "num_images = 5\n",
    "total = len(all_masks)\n",
    "\n",
    "num_train = int(0.99*total)\n",
    "num_test = total-num_train\n",
    "print(total)\n",
    "print(num_train)\n",
    "print(num_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create empty arrays that are going to contain all our data. Note that this works only if the data are not too large or you have a computer with **a lot of RAM**. The alternative is to use a more complex approach using Python generators, which are going to *serve* images sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = np.ndarray((num_train, image_rows, image_cols,channels), dtype=np.float64)\n",
    "imgs_mask = np.ndarray((num_train, image_rows, image_cols), dtype=np.uint8)\n",
    "imgs_test = np.ndarray((num_test, image_rows, image_cols,channels), dtype=np.float64)\n",
    "imgs_id = np.ndarray((num_test, ), dtype=np.int32)\n",
    "imgs_weight = np.ndarray((num_train, image_rows, image_cols), dtype=np.uint8)\n",
    "imgs_weight[:]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we fill up our containers. Note that they have to be in special shapes to be fed correctly to the network. Also, in addition to our images and masks, we have so-called weights. This is an image that is going to assign more importance to certain regions. This is important for example if one category of pixels appears much less than another, like in our case nuclei vs. background.\n",
    "\n",
    "Note also that we correct all images by normalizing them to avoid extreme values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for counter in range(total):\n",
    "    if counter<num_train:\n",
    "        imgs[counter] = all_images[counter][..., np.newaxis]\n",
    "        imgs_mask[counter] = all_masks[counter]\n",
    "        imgs_weight[counter] = 10*all_masks[counter]+1\n",
    "    else:\n",
    "        imgs_test[counter-num_train] = all_images[counter][..., np.newaxis]\n",
    "        imgs_id[counter-num_train] = counter-num_train\n",
    "\n",
    "mean_val = np.mean(imgs)\n",
    "imgs = imgs - mean_val\n",
    "std_val = np.std(imgs)\n",
    "imgs = imgs/std_val\n",
    "\n",
    "np.save('MyData/DL/'+'imgs_train.npy', imgs)\n",
    "np.save('MyData/DL/'+'imgs_mask_train.npy', imgs_mask.reshape((num_train,image_rows*image_cols)))\n",
    "np.save('MyData/DL/'+'imgs_test.npy', imgs_test)\n",
    "np.save('MyData/DL/'+'imgs_id_test.npy', imgs_id)\n",
    "np.save('MyData/DL/'+'imgs_weight_train.npy', imgs_weight.reshape((num_train,image_rows*image_cols)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.2 Training the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can import our small deep learning module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import deeplearning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can run the training of our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 224 samples, validate on 56 samples\n",
      "Epoch 1/10\n",
      "224/224 [==============================] - 30s 134ms/step - loss: 0.9972 - dice_coef: 0.3031 - val_loss: 0.4593 - val_dice_coef: 0.3932\n",
      "Epoch 2/10\n",
      "224/224 [==============================] - 28s 126ms/step - loss: 0.3411 - dice_coef: 0.5606 - val_loss: 0.2240 - val_dice_coef: 0.6404\n",
      "Epoch 3/10\n",
      "224/224 [==============================] - 22s 97ms/step - loss: 0.1893 - dice_coef: 0.7272 - val_loss: 0.1442 - val_dice_coef: 0.7747\n",
      "Epoch 4/10\n",
      "224/224 [==============================] - 21s 94ms/step - loss: 0.1358 - dice_coef: 0.8080 - val_loss: 0.1444 - val_dice_coef: 0.7878\n",
      "Epoch 5/10\n",
      "224/224 [==============================] - 21s 95ms/step - loss: 0.1316 - dice_coef: 0.8231 - val_loss: 0.1267 - val_dice_coef: 0.7974\n",
      "Epoch 6/10\n",
      "224/224 [==============================] - 21s 95ms/step - loss: 0.1203 - dice_coef: 0.8308 - val_loss: 0.1128 - val_dice_coef: 0.8404\n",
      "Epoch 7/10\n",
      "224/224 [==============================] - 21s 95ms/step - loss: 0.1135 - dice_coef: 0.8412 - val_loss: 0.1132 - val_dice_coef: 0.8246\n",
      "Epoch 8/10\n",
      "224/224 [==============================] - 21s 95ms/step - loss: 0.1094 - dice_coef: 0.8488 - val_loss: 0.1011 - val_dice_coef: 0.8491\n",
      "Epoch 9/10\n",
      "224/224 [==============================] - 21s 95ms/step - loss: 0.1008 - dice_coef: 0.8589 - val_loss: 0.1094 - val_dice_coef: 0.8190\n",
      "Epoch 10/10\n",
      "224/224 [==============================] - 32s 144ms/step - loss: 0.1024 - dice_coef: 0.8557 - val_loss: 0.1024 - val_dice_coef: 0.8686\n"
     ]
    }
   ],
   "source": [
    "image_rows = 64\n",
    "image_cols = 64\n",
    "\n",
    "#deeplearning.nuclei_train('MyData/DL/', image_rows,image_cols, dims=1, batch_size = 10, epochs = 100, weights = None)\n",
    "deeplearning.nuclei_train('MyData/DL/', image_rows,image_cols, dims=1, batch_size = 10, epochs = 10, weights = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15.3 Using the trained network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load an image that we did **not** use for training and select a 512x512 region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = data.pages[143].asarray()[0:512,0:512]\n",
    "im_float = image.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load again the network and say what the input size will be. Then **most importantly**, we use the weights that we just trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = deeplearning.get_unet(1,512,512)\n",
    "model.load_weights('MyData/DL/weights.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We correct now this single picture **with the same factors** used for the training set, so that it is in the same *state*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_test = im_float.astype('float32')\n",
    "imgs_test = imgs_test\n",
    "imgs_test = imgs_test - mean_val\n",
    "imgs_test = imgs_test/std_val\n",
    "plt.imshow(imgs_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we reshape it to fit into the network and use the predict() function to generate a prediction for each pixel to be foreground or background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_test = imgs_test[np.newaxis,...,np.newaxis]\n",
    "imgs_mask_test = model.predict(imgs_test, verbose=1)\n",
    "imgs_mask_test = np.reshape(imgs_mask_test,imgs_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can plot the resulting image, which has values from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(imgs_mask_test[0,:,:,0], vmin = 0, vmax = 1, cmap= 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now set a threshold for what should be considerd foreground to generate a mask, and compare to the previous segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nuclei = detect_nuclei(image)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(imgs_mask_test[0,:,:,0]>0.9, cmap = 'gray')\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(nuclei[0:512,0:512], cmap = 'gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
